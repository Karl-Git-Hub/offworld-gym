{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed168c9",
   "metadata": {},
   "source": [
    " # **Soft Actor-Critic**\n",
    "Implemented in TensorFlow 2.6 with TF-Agents.\n",
    "**Soft Actor-Critic** algorithm learns not only rewards, but also tries to maximize the entropy of its actions. In other words, it tries to be as unpredicatable as possible while still getting as many rewards as possible. This encourages the agent to explore the environment, which speeds up training, and makes it less likely to repeatedly execute same action when DQN produces imperfect estimates. This should lead to amazing sample efficiency.\n",
    "\n",
    "[Soft Actor-Critic Algorithms and Applications](https://arxiv.org/abs/1812.05905)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ece10",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59843d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from offworld_gym.envs.common.channels import Channels\n",
    "from offworld_gym.envs.common.enums import AlgorithmMode, LearningType\n",
    "\n",
    "import silence_tensorflow.auto\n",
    "import os\n",
    "import logging\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import datetime\n",
    "import certifi\n",
    "import urllib3\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.sac import sac_agent \n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.train import actor\n",
    "from tf_agents.train import learner\n",
    "from tf_agents.train.utils import spec_utils\n",
    "from tf_agents.train.utils import strategy_utils\n",
    "from tf_agents.train.utils import train_utils\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247c51bc",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "### Offworld-Gym env parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aae33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "real = False                             # True = Real environment | False = Simulated docker environment\n",
    "experiment_name = 'Buf_1'                # For https://gym.offworld.ai/myexperiments experiment name\n",
    "resume_experiment = True                 # Resume training? For real env\n",
    "delete_checkpoints = False               # Delete model checkpoints for 'experiment_name' experiment\n",
    "\n",
    "learning_type = LearningType.END_TO_END  # Description of training method\n",
    "algorithm_mode = AlgorithmMode.TRAIN     # .TEST or .TRAIN  (TEST needed for Offworld leaderboard)\n",
    "channel_type = Channels.DEPTH_ONLY       # Which sensors to use: .RGB_ONLY or .DEPTH_ONLY or .RGBD\n",
    "\n",
    "# Access token for Real environment from https://gym.offworld.ai/account\n",
    "import my_Gym_token # I have this in separate file, but it can be added as string\n",
    "os.environ['OFFWORLD_GYM_ACCESS_TOKEN'] = my_Gym_token.is_secret # 'insert_as_a_string'\n",
    "\n",
    "# Project root folder. If path unknown, run in terminal: pwm\n",
    "os.environ['OFFWORLD_GYM_ROOT'] = '/home/karlaru/PycharmProjects/offworld-gym'\n",
    "\n",
    "# Python environment used for this project (eg miniconda env). If path unknown, run in terminal: which python\n",
    "os.environ['PYTHONPATH'] = '/home/karlaru/miniconda3/envs/offworld-karl/bin/python'\n",
    "\n",
    "# Load right environment\n",
    "if real:\n",
    "    env_name = 'OffWorldMonolithContinuousReal-v0'  \n",
    "else:\n",
    "    env_name = 'OffWorldDockerMonolithContinuousSim-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc287dca",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e3f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show only INFO messages\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
    "\n",
    "# Disable connection warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361e6197",
   "metadata": {},
   "source": [
    "### Model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276b72ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tempdir for model checkpoints\n",
    "tempdir = 'models/' + experiment_name + '/'\n",
    "\n",
    "# Clean checkpoints if clean training start wanted\n",
    "if delete_checkpoints == True:\n",
    "    try:\n",
    "        shutil.rmtree(tempdir)\n",
    "        logging.info(f'Successfully deleted checkpoints in {tempdir}')\n",
    "    except:\n",
    "        logging.info(f'{tempdir} already empty')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e05de",
   "metadata": {},
   "source": [
    "### Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c6629f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Buffer tempdir: data/sim_buffer/\n"
     ]
    }
   ],
   "source": [
    "# Steps to train with random policy to fill buffer (when 0 then skipped)\n",
    "steps_random_training = 0\n",
    "\n",
    "# How often to save random policy training to file (<=steps_random_training)\n",
    "steps_save_random = 10000\n",
    "\n",
    "# Max number of steps to keep in replay buffer\n",
    "replay_max_length = 50000 # == 14.3GB file\n",
    "\n",
    "# Patch size to get from buffer for one network training interation\n",
    "sample_batch_size = 64\n",
    "\n",
    "# Update replay buffer file with new data while training model?\n",
    "update_replay = True\n",
    "\n",
    "# Random replay buffer is in project folder, so it doesn't get deleted and can be reused\n",
    "if real:\n",
    "    rb_tempdir = 'data/real_buffer/'\n",
    "else:\n",
    "    rb_tempdir = 'data/sim_buffer/'\n",
    "    \n",
    "    \n",
    "logging.info(f'Buffer tempdir: {rb_tempdir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b587fbd",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "010f1ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episodes to collect before retraining network\n",
    "episodes_in_patch = 25\n",
    "\n",
    "# Total episodes to run\n",
    "episodes_to_train = 100000\n",
    "\n",
    "# Iterations to do each time after new data collection\n",
    "training_iterations = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020843e",
   "metadata": {},
   "source": [
    "# Computation distribution strategy\n",
    "\n",
    "Enables running computations on one or more devices in a way that model definition code can remain unchanged when running on different hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6bb7196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Using 1 GPU\n"
     ]
    }
   ],
   "source": [
    "# Number of GPU-s available in current machine\n",
    "num_GPUs = len(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# If no GPU-s available, use CPU\n",
    "if num_GPUs == 0:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "    logging.info('No GPUs available. Using only CPU.')\n",
    "\n",
    "# If one GPU available\n",
    "elif num_GPUs == 1:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    logging.info('Using 1 GPU')\n",
    "\n",
    "# If more than one GPU available, mirror data for compute in parallel\n",
    "else:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    logging.info('Using multible GPUs' + str(num_GPUs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a26fb",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "In Reinforcement Learning (RL), an environment represents the task or problem to be solved. TF-Agents has suites for loading environments such as the OpenAI Gym. OpenAI Gym is written in pure Python. This is converted to TensorFlow using the TFPyEnvironment wrapper. The original environment's API uses Numpy arrays. The TFPyEnvironment converts these to Tensors to make it compatible with Tensorflow agents and policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f3cf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:container_id is 23e7072f6d057af2ec2967440aadd5b773f94a63bb268f4d08d74ea5d39d01a1\n",
      "INFO:\u001b[32mFor visualization of simulation, visit gzweb server at http://127.0.1.1:49175\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if real == False:\n",
    "    env = suite_gym.wrap_env(gym_env=gym.make(env_name, channel_type=channel_type))\n",
    "\n",
    "else:\n",
    "    env = suite_gym.wrap_env(gym_env=gym.make(env_name, \n",
    "                                              experiment_name=experiment_name,\n",
    "                                              resume_experiment=resume_experiment,\n",
    "                                              channel_type=channel_type, \n",
    "                                              learning_type=learning_type,\n",
    "                                              algorithm_mode=algorithm_mode\n",
    "                                              )) \n",
    "# Wrap Gym env into TF env\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10b20931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tensor specs\n",
    "observation_spec, action_spec, time_step_spec = spec_utils.get_tensor_specs(tf_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8186d1a3",
   "metadata": {},
   "source": [
    "### Observation space\n",
    "\n",
    "One observation is one frame from depth camera senor. Sensor resolution is 240x320 pix. Values for each pix range from 0 to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc606fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(1, 240, 320, 1), dtype=tf.float32, name='observation', minimum=array(0., dtype=float32), maximum=array(255., dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3dc926",
   "metadata": {},
   "source": [
    "### Action space\n",
    "\n",
    "Robot movement command is defined by 2 element vector and has continuous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "000f82f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(2,), dtype=tf.float32, name='action', minimum=array([-0.7, -2.5], dtype=float32), maximum=array([0.7, 2.5], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d89bf1",
   "metadata": {},
   "source": [
    "### Time step\n",
    "\n",
    "A TimeStep contains the data emitted by an environment at each step of interaction. A TimeStep holds a step_type, an observation (typically a NumPy array or a dict or list of arrays), and an associated reward and discount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d2f423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))\n",
      "BoundedTensorSpec(shape=(1, 240, 320, 1), dtype=tf.float32, name='observation', minimum=array(0., dtype=float32), maximum=array(255., dtype=float32))\n",
      "TensorSpec(shape=(), dtype=tf.float32, name='reward')\n",
      "TensorSpec(shape=(), dtype=tf.int32, name='step_type')\n"
     ]
    }
   ],
   "source": [
    "print(time_step_spec.discount)\n",
    "print(time_step_spec.observation)\n",
    "print(time_step_spec.reward)\n",
    "print(time_step_spec.step_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd0af4",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6332b",
   "metadata": {},
   "source": [
    "### Critic\n",
    "Gives value estimates for Q(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c890d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  critic_net = critic_network.CriticNetwork(\n",
    "        (observation_spec, action_spec),\n",
    "        observation_conv_layer_params=((64,20,3), (64,10,2), (32,5,1), (16,3,1)),\n",
    "        observation_fc_layer_params=(512, 256, 128, 64, 32, 16, 8), \n",
    "        observation_dropout_layer_params=None,\n",
    "        action_fc_layer_params=(512,256, 128, 64), \n",
    "        action_dropout_layer_params=None,\n",
    "        joint_fc_layer_params=(512,256, 128, 64),\n",
    "        joint_dropout_layer_params=None,\n",
    "        activation_fn=tf.nn.relu, \n",
    "        output_activation_fn=tf.keras.activations.relu, \n",
    "        name='CriticNetwork')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c660a2",
   "metadata": {},
   "source": [
    "### Actor\n",
    "Generates actions for given observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "198ebff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "        observation_spec,\n",
    "        action_spec,\n",
    "        preprocessing_layers = None,\n",
    "        preprocessing_combiner=None, \n",
    "        conv_layer_params=((64,20,3), (64,10,2), (32,5,1), (16,3,1)),\n",
    "        fc_layer_params=(512, 256, 128, 64, 32, 16, 8),\n",
    "        dropout_layer_params=None,\n",
    "        kernel_initializer=None,\n",
    "        activation_fn=tf.keras.activations.relu,\n",
    "        continuous_projection_net=tanh_normal_projection_network.TanhNormalProjectionNetwork,\n",
    "        name='ActorDistributionNetwork')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e01114f",
   "metadata": {},
   "source": [
    "### Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffe9960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  train_step = train_utils.create_train_step()\n",
    "  tf_agent = sac_agent.SacAgent(\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        critic_network=critic_net,\n",
    "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=0.001),\n",
    "        actor_network=actor_net,\n",
    "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=0.001),\n",
    "        alpha_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=0.001),\n",
    "        actor_loss_weight = 1.0,\n",
    "        critic_loss_weight = 0.5,\n",
    "        alpha_loss_weight = 1.0,\n",
    "        target_update_tau=0.01,\n",
    "        target_update_period=1,\n",
    "        td_errors_loss_fn=tf.math.squared_difference,\n",
    "        gamma=0.99,\n",
    "        reward_scale_factor=1.0,\n",
    "        initial_log_alpha = 0.1,\n",
    "        use_log_alpha_in_alpha_loss = False,\n",
    "        target_entropy = -2, # -0.8 as good as random in 4k episodes\n",
    "        gradient_clipping = None,\n",
    "        debug_summaries = False,\n",
    "        summarize_grads_and_vars = False,\n",
    "        train_step_counter=train_step,\n",
    "        name='Agent')\n",
    "\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45856d11",
   "metadata": {},
   "source": [
    "### Critic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fbb658f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CriticNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "observation_encoding/conv2d  multiple                  25664     \n",
      "_________________________________________________________________\n",
      "observation_encoding/conv2d  multiple                  409664    \n",
      "_________________________________________________________________\n",
      "observation_encoding/conv2d  multiple                  51232     \n",
      "_________________________________________________________________\n",
      "observation_encoding/conv2d  multiple                  4624      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "observation_encoding/dense ( multiple                  8847872   \n",
      "_________________________________________________________________\n",
      "observation_encoding/dense ( multiple                  131328    \n",
      "_________________________________________________________________\n",
      "observation_encoding/dense ( multiple                  32896     \n",
      "_________________________________________________________________\n",
      "observation_encoding/dense ( multiple                  8256      \n",
      "_________________________________________________________________\n",
      "observation_encoding/dense ( multiple                  2080      \n",
      "_________________________________________________________________\n",
      "observation_encoding/dense ( multiple                  528       \n",
      "_________________________________________________________________\n",
      "observation_encoding/dense ( multiple                  136       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "action_encoding/dense (Dense multiple                  1536      \n",
      "_________________________________________________________________\n",
      "action_encoding/dense (Dense multiple                  131328    \n",
      "_________________________________________________________________\n",
      "action_encoding/dense (Dense multiple                  32896     \n",
      "_________________________________________________________________\n",
      "action_encoding/dense (Dense multiple                  8256      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "joint_mlp/dense (Dense)      multiple                  37376     \n",
      "_________________________________________________________________\n",
      "joint_mlp/dense (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "joint_mlp/dense (Dense)      multiple                  32896     \n",
      "_________________________________________________________________\n",
      "joint_mlp/dense (Dense)      multiple                  8256      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  65        \n",
      "=================================================================\n",
      "Total params: 9,898,217\n",
      "Trainable params: 9,898,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e288035c",
   "metadata": {},
   "source": [
    "### Actor net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11852a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ActorDistributionNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncodingNetwork (EncodingNet multiple                  9514280   \n",
      "_________________________________________________________________\n",
      "TanhNormalProjectionNetwork  multiple                  36        \n",
      "=================================================================\n",
      "Total params: 9,514,316\n",
      "Trainable params: 9,514,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81946537",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4b8e83",
   "metadata": {},
   "source": [
    "### Tensorboard logs for random buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8efbd01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder for random policy logs\n",
    "log_dir = 'logs/RANDOM'\n",
    "\n",
    "# Real and Sim in different subfolders\n",
    "if real:\n",
    "    log_dir += '/REAL/'\n",
    "else:\n",
    "    log_dir += '/SIM/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e7bd6",
   "metadata": {},
   "source": [
    "### Observer (called after each step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "433d2dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Agents metrics\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(buffer_size=episodes_in_patch),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(buffer_size=episodes_in_patch)]\n",
    "\n",
    "# Custom observer for random policy Tensorboard logging\n",
    "class Observer:\n",
    "    def __init__(self):\n",
    "        # Initialize in class to avoid empty event files\n",
    "        self.summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    \n",
    "    def __call__(self, trajectory):\n",
    "        with self.summary_writer.as_default(step = train_metrics[0].result().numpy()):             \n",
    "            if train_metrics[0].result().numpy() % episodes_in_patch == 0:\n",
    "                # Write to Tensorboard log folder\n",
    "                tf.summary.scalar(f'Avg Reward per episode', train_metrics[2].result().numpy())\n",
    "                tf.summary.scalar(f'Avg Steps per episode', train_metrics[3].result().numpy())\n",
    "              \n",
    "            print(f\"\\rTotal steps: {train_metrics[1].result().numpy()} in {train_metrics[0].result().numpy()} episodes\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a2b8b4",
   "metadata": {},
   "source": [
    "# Replay buffer\n",
    "\n",
    "Store data about previous training experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65e1bd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create replay buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec = tf_agent.collect_data_spec,\n",
    "    batch_size = 1,                 # Replays are stored one at the time\n",
    "    max_length = replay_max_length, # Total buffer size\n",
    "    scope='TFUniformReplayBuffer',\n",
    "    device='cpu:0',                 # Use CPU for data storage and compute (GPU needed for network training)\n",
    "    dataset_drop_remainder=True,    \n",
    "    dataset_window_shift=None, \n",
    "    stateful_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a740b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Loaded 50000 steps from data/sim_buffer/\n"
     ]
    }
   ],
   "source": [
    "# Try loading replay buffer from a file\n",
    "try:\n",
    "    tf.train.Checkpoint(replay_buffer=replay_buffer).restore(rb_tempdir+ '-1')\n",
    "    logging.info(f\"Loaded {replay_buffer.gather_all().action[0].shape[0]} steps from {rb_tempdir}\")\n",
    "except:\n",
    "    logging.info(f\"No random policy replay buffer steps found in {rb_tempdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c866293",
   "metadata": {},
   "source": [
    "#### Reading data from buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "685b322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from replay buffer\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=sample_batch_size,\n",
    "    num_parallel_calls = 1,\n",
    "    num_steps=2).prefetch(50)\n",
    "\n",
    "experience_dataset_fn = lambda: dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06404998",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99593153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f660075006a90a3f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f660075006a90a3f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8df58d1",
   "metadata": {},
   "source": [
    "# Random training\n",
    "\n",
    "Random training helps to converge faster. It is filling replay buffer (database) with sample data. Initial SAC policy could be worse than random!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcbbe3e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Using replay from file. No new Steps\n"
     ]
    }
   ],
   "source": [
    "if steps_random_training > 0:\n",
    "    # Use random policy\n",
    "    initial_collect_policy = random_tf_policy.RandomTFPolicy(action_spec = tf_env.action_spec(),\n",
    "                                                          time_step_spec = tf_env.time_step_spec())\n",
    "    # Use step driver\n",
    "    inital_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        tf_env, \n",
    "        initial_collect_policy, \n",
    "        observers = [replay_buffer.add_batch, Observer()] + train_metrics, \n",
    "        num_steps = steps_save_random)\n",
    "    \n",
    "    for _ in range(int(steps_random_training/steps_save_random)):\n",
    "        \n",
    "        # Do 'steps_save_random' steps\n",
    "        inital_driver.run()\n",
    "        \n",
    "        # Save replay buffer\n",
    "        tf.train.Checkpoint(replay_buffer=replay_buffer).save(rb_tempdir)\n",
    "        logging.info(f\"Saved replay buffer at {replay_buffer.gather_all().action[0].shape[0]} steps\")\n",
    "else:\n",
    "    logging.info(f\"Using replay from file. No new Steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a4ada",
   "metadata": {},
   "source": [
    "### Tensorboard logs for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed87332",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9104fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset TF metrics after random buffer filling\n",
    "for i in range(4):\n",
    "    train_metrics[i].reset()\n",
    "    \n",
    "# Folder for training logs\n",
    "log_dir = 'logs/' + experiment_name\n",
    "\n",
    "# Place sim logs in sim subfolder\n",
    "if real:\n",
    "    log_dir += '/REAL/'\n",
    "else:\n",
    "    log_dir += '/SIM/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d1b799",
   "metadata": {},
   "source": [
    "# Collect driver\n",
    "\n",
    "Driver for running a policy in an environment. Does steps until num_episodes episodes is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63ccfc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use episode driver (nr of steps per episode is limited by env)\n",
    "collect_actor = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env, \n",
    "    py_tf_eager_policy.PyTFEagerPolicy(tf_agent.collect_policy, use_tf_function=True),\n",
    "    observers = [replay_buffer.add_batch, Observer()] + train_metrics,\n",
    "    num_episodes = episodes_in_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39db5603",
   "metadata": {},
   "source": [
    "# Learner\n",
    "\n",
    "Learner loads checkpoint from tempdir if available, so learning can be resumed from saved point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af62f915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Checkpoint available: models/Buf_1/train/checkpoints/ckpt-98020\n"
     ]
    }
   ],
   "source": [
    "agent_learner = learner.Learner(\n",
    "    tempdir,\n",
    "    train_step, \n",
    "    tf_agent, \n",
    "    experience_dataset_fn,\n",
    "    checkpoint_interval=training_iterations,\n",
    "    summary_interval=100000, \n",
    "    max_checkpoints_to_keep=2,\n",
    "    strategy=strategy,\n",
    "    run_optimizer_variable_init=True)\n",
    "\n",
    "# in debug INFO: ckpt-xxxx <- last number shows how many iterations have been done for current experiment name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe00c8a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02296a27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Saved checkpoint: models/Buf_1/train/checkpoints/ckpt-98520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 235 in 25 episodes"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Saved checkpoint: models/Buf_1/train/checkpoints/ckpt-99020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 490 in 50 episodes"
     ]
    }
   ],
   "source": [
    "for i in range(int(episodes_to_train/episodes_in_patch)):\n",
    "     \n",
    "    # Only learning when mode = TRAIN\n",
    "    if  algorithm_mode == AlgorithmMode.TRAIN:\n",
    "        \n",
    "        # Update policy on replay buffer data.\n",
    "        agent_learner.run(iterations=training_iterations)    \n",
    "    \n",
    "    \n",
    "    # Collect new data\n",
    "    collect_actor.run()\n",
    "    \n",
    "    # Backup replay buffer into data folder\n",
    "    if i % 20 == 0 and i > 0 and update_replay:\n",
    "        tf.train.Checkpoint(replay_buffer=replay_buffer).save(rb_tempdir)\n",
    "        logging.info(f\"Updated replay buffer!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
