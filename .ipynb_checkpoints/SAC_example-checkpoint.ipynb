{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed168c9",
   "metadata": {},
   "source": [
    " # **Soft Actor-Critic**\n",
    "Implemented in TensorFlow 2.6 with TF-Agents.\n",
    "**Soft Actor-Critic** algorithm learns not only rewards, but also tries to maximize the entropy of its actions. In other words, it tries to be as unpredicatable as possible while still getting as many rewards as possible. This encourages the agent to explore the environment, which speeds up training, and makes it less likely to repeatedly execute same action when DQN produces imperfect estimates. This should lead to amazing sample efficiency.\n",
    "\n",
    "[Soft Actor-Critic Algorithms and Applications](https://arxiv.org/abs/1812.05905)\n",
    "\n",
    "### How to get started\n",
    "\n",
    "**In simulation environment**\n",
    "\n",
    "- collect 50k steps with random policy to get a baseline to compare against\n",
    "- find network that looks the most promising and train 100k iterations on collected random buffer with learning rate 0.001\n",
    "- use discount factor (gamma) 0.98 for future rewards. This forces agent to do less steps to get the reward, which in turn increases replay buffer rewards density and therefore learning becomes more efficent.\n",
    "- after 100k-300k iterations decrease learning rate to 0.0001 to get more stable results\n",
    "- check model summaries with Tensorboard (tensorboard --logdir=models) to fine-tune new steps vs training iteration steps. If losses increase then more iterations is needed.\n",
    "\n",
    "**In real environment**\n",
    "- Do transfer learning on sim network\n",
    "- Start collecting steps with SIM trained Agent to get SIM 2 REAL accuracy\n",
    "- After 25 episodes start training Agent in real environment\n",
    "- Learning rate should be higher, because there are less steps in real env\n",
    "\n",
    "### Comparison \"Micro-net\" spec\n",
    "\n",
    "Agent with this parameters learn faster, but not that stable. Maby learning rate was too high...\n",
    "\n",
    "- conv_layer_params = (4,7,4), (1,4,1)\n",
    "- fc_layer_params = (16, 16, 8)\n",
    "- dropout_layer_params = (0.25, 0.25, 0.125)\n",
    "- action_fc_layer_params = (256, 256, 256, 256)\n",
    "- action_dropout_layer_params = (0.25, 0.25, 0.125, 0.25)\n",
    "- joint_fc_layer_params = (32, 16, 16, 4)\n",
    "- joint_dropout_layer_params = (0.25, 0.25, 0.125, 0.25)\n",
    "- activation_fn = tf.keras.activations.swish\n",
    "\n",
    "### Other remarks\n",
    "\n",
    "- activation functrion ReLU doesn't seem to work, because when inputs approach zero or are negative, the gradient of the function becomes zero. The network cannot perform backpropagation and cannot learn. More info: [Comparison of Reinforcement Learning Activation Functions to Improve the Performance of the Racing Game Learning Agent](https://s3.ap-northeast-2.amazonaws.com/journal-home/journal/jips/fullText/477/jips_v16n5_7.pdf)\n",
    "\n",
    "- when chaning network layers, change experiment name or delete model checkpoints in models/'changed_model/checkpoints. Otherwise tensor shape mismatch is rised.\n",
    "\n",
    "- when learning error 'loss in inf or nan' occured, changing network learning rate might help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ece10",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59843d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from offworld_gym.envs.common.channels import Channels\n",
    "from offworld_gym.envs.common.enums import AlgorithmMode, LearningType\n",
    "\n",
    "import silence_tensorflow.auto\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import datetime\n",
    "import certifi\n",
    "import urllib3\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.sac import sac_agent \n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.train import actor\n",
    "from tf_agents.train import learner\n",
    "from tf_agents.train.utils import spec_utils\n",
    "from tf_agents.train.utils import strategy_utils\n",
    "from tf_agents.train.utils import train_utils\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247c51bc",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "### Offworld-Gym env parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aae33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "real = True                       # True = Real environment | False = Simulated docker environment\n",
    "experiment_name = 'Macro_net'     # Experiment name\n",
    "resume_experiment = True          # Resume training. When going from SIM to REAL, then transferlearning\n",
    "\n",
    "learning_type = LearningType.END_TO_END  # Description of training method\n",
    "algorithm_mode = AlgorithmMode.TRAIN     # .TEST or .TRAIN  (TEST needed for Offworld leaderboard)\n",
    "channel_type = Channels.DEPTH_ONLY       # Which sensors to use: .RGB_ONLY or .DEPTH_ONLY or .RGBD\n",
    "\n",
    "# Access token for Real environment from https://gym.offworld.ai/account\n",
    "import my_Gym_token # I have this in separate file, but it can be added as string\n",
    "os.environ['OFFWORLD_GYM_ACCESS_TOKEN'] = my_Gym_token.is_secret # 'insert_as_a_string'\n",
    "\n",
    "# Project root folder. If path unknown, run in terminal: pwm\n",
    "os.environ['OFFWORLD_GYM_ROOT'] = '/home/karlaru/PycharmProjects/offworld-gym'\n",
    "\n",
    "# Python environment used for this project (eg miniconda env). If path unknown, run in terminal: which python\n",
    "os.environ['PYTHONPATH'] = '/home/karlaru/miniconda3/envs/offworld-karl/bin/python'\n",
    "\n",
    "# Load right environment\n",
    "if real:\n",
    "    env_name = 'OffWorldMonolithContinuousReal-v0'  \n",
    "else:\n",
    "    env_name = 'OffWorldDockerMonolithContinuousSim-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc287dca",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e3f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show only INFO messages\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
    "\n",
    "# Disable connection security warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361e6197",
   "metadata": {},
   "source": [
    "### Model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276b72ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Model tempdir: models/Macro_net/\n"
     ]
    }
   ],
   "source": [
    "# Tempdir for model checkpoints\n",
    "tempdir = 'models/' + experiment_name + '/'\n",
    "logging.info(f'Model tempdir: {tempdir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e05de",
   "metadata": {},
   "source": [
    "### Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c6629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max number of steps to keep in replay buffer\n",
    "replay_max_length = 50000 # 14.3GB file, but uses at least triple the RAM when starting training or saving to file\n",
    "\n",
    "# Fill replay buffer with random steps (recomended in sim environment)\n",
    "random_replay_fill = False\n",
    "\n",
    "# Patch size to get from buffer for one network training interation\n",
    "sample_batch_size = 128\n",
    "\n",
    "# Load replay buffer seed from file for quicker training?\n",
    "buffer_from_file = True           # start training with \"warm\" (with previously made steps) buffer\n",
    "save_buffer_to_file = True        # save last buffer state to a file so training can be continued where left of\n",
    "buffer_save_interval = 9999999999 # save buffer after x interval, when huge then save only after training end\n",
    "\n",
    "# Replay buffer directory\n",
    "if real:\n",
    "    rb_tempdir = 'data/real_buffer/'\n",
    "else:\n",
    "    rb_tempdir = 'data/sim_buffer/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b587fbd",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "010f1ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of patches to run 'range(1, patches_to_run+1)'\n",
    "patches_to_run = 1000\n",
    "\n",
    "# Start collecting new steps after patch number 'current_patch_nr > start_collecting' (before only training)\n",
    "start_collecting = 0\n",
    "\n",
    "# Episodes to collect before retraining network\n",
    "episodes_in_patch = 5      # basis for calculating Tensorboard stats 'tensorboard --logdir=logs'\n",
    "times_to_collect = 4       # collect 25 x 80 = 2k episodes (~10k steps) before retraining\n",
    "\n",
    "# Training iterations after collecting patch\n",
    "training_iterations = 250   # basis for calculating Tensorboard stats 'tensorboard --logdir=models'\n",
    "times_to_iterate = 20       # train 250 x 20 = 5k iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020843e",
   "metadata": {},
   "source": [
    "# Computation distribution strategy\n",
    "\n",
    "Enables running computations on one or more devices in a way that model definition code can remain unchanged when running on different hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6bb7196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Using 1 GPU\n"
     ]
    }
   ],
   "source": [
    "# Number of GPU-s available in current machine\n",
    "num_GPUs = len(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# If no GPU-s available, use CPU\n",
    "if num_GPUs == 0:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "    logging.info('No GPUs available. Using only CPU.')\n",
    "\n",
    "# If one GPU available\n",
    "elif num_GPUs == 1:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    logging.info('Using 1 GPU')\n",
    "\n",
    "# If more than one GPU available, mirror data for compute in parallel\n",
    "else:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    logging.info('Using multible GPUs' + str(num_GPUs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a26fb",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "In Reinforcement Learning (RL), an environment represents the task or problem to be solved. TF-Agents has suites for loading environments such as the OpenAI Gym. OpenAI Gym is written in pure Python. This is converted to TensorFlow using the TFPyEnvironment wrapper. The original environment's API uses Numpy arrays. The TFPyEnvironment converts these to Tensors to make it compatible with Tensorflow agents and policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f3cf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 01:21:09,122 - offworld_gym - INFO - Environment has been initiated.\n",
      "INFO:Environment has been initiated.\n",
      "2021-12-06 01:21:09,124 - offworld_gym - INFO - Environment has been started.\n",
      "INFO:Environment has been started.\n",
      "/home/karlaru/.local/lib/python3.6/site-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  \"Box bound precision lowered by casting to {}\".format(self.dtype)\n",
      "2021-12-06 01:21:09,125 - offworld_gym - INFO - Waiting to connect to the environment server.\n",
      "INFO:Waiting to connect to the environment server.\n",
      "2021-12-06 01:21:13,448 - offworld_gym - INFO - Experiment has been resumed.\n",
      "INFO:Experiment has been resumed.\n",
      "2021-12-06 01:21:13,452 - offworld_gym - INFO - The environment server is running.\n",
      "INFO:The environment server is running.\n"
     ]
    }
   ],
   "source": [
    "if real == False:\n",
    "    env = suite_gym.wrap_env(gym_env=gym.make(env_name, channel_type=channel_type))\n",
    "\n",
    "else:\n",
    "    env = suite_gym.wrap_env(gym_env=gym.make(env_name, \n",
    "                                              experiment_name=experiment_name,\n",
    "                                              resume_experiment=resume_experiment,\n",
    "                                              channel_type=channel_type, \n",
    "                                              learning_type=learning_type,\n",
    "                                              algorithm_mode=algorithm_mode\n",
    "                                              )) \n",
    "# Wrap Gym env into TF env\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10b20931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tensor specs\n",
    "observation_spec, action_spec, time_step_spec = spec_utils.get_tensor_specs(tf_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8186d1a3",
   "metadata": {},
   "source": [
    "### Observation space\n",
    "\n",
    "One observation is one frame from depth camera senor. Sensor resolution is 240x320 pix. Values for each pix range from 0 to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc606fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(1, 240, 320, 1), dtype=tf.float32, name='observation', minimum=array(0., dtype=float32), maximum=array(255., dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3dc926",
   "metadata": {},
   "source": [
    "### Action space\n",
    "\n",
    "Robot movement command is defined by 2 element vector and has continuous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "000f82f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(2,), dtype=tf.float32, name='action', minimum=array([-0.7, -2.5], dtype=float32), maximum=array([0.7, 2.5], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d89bf1",
   "metadata": {},
   "source": [
    "### Time step\n",
    "\n",
    "A TimeStep contains the data emitted by an environment at each step of interaction. A TimeStep holds a step_type, an observation (typically a NumPy array or a dict or list of arrays), and an associated reward and discount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d2f423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))\n",
      "BoundedTensorSpec(shape=(1, 240, 320, 1), dtype=tf.float32, name='observation', minimum=array(0., dtype=float32), maximum=array(255., dtype=float32))\n",
      "TensorSpec(shape=(), dtype=tf.float32, name='reward')\n",
      "TensorSpec(shape=(), dtype=tf.int32, name='step_type')\n"
     ]
    }
   ],
   "source": [
    "print(time_step_spec.discount)\n",
    "print(time_step_spec.observation)\n",
    "print(time_step_spec.reward)\n",
    "print(time_step_spec.step_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd0af4",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6332b",
   "metadata": {},
   "source": [
    "### Critic\n",
    "Gives value estimates for Q(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3b6f279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Observation layer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 78, 105, 16)       800       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 37, 51, 8)         3208      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 48, 1)         129       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1632)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               209024    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 244,297\n",
      "Trainable params: 244,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Observation layer planner for critic and actor (to see output shapes) Dropout layers are not displayed.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input\n",
    "\n",
    "model = Sequential(name='Observation layer')\n",
    "model.add(Input((240, 320, 1)))\n",
    "model.add(Conv2D(16, 7, strides=3, activation='swish'))\n",
    "model.add(Conv2D(8, 5, strides=2, activation='swish')) \n",
    "model.add(Conv2D(1, 4, strides=1, activation='swish'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='swish'))\n",
    "model.add(Dense(64, activation='swish'))\n",
    "model.add(Dense(64, activation='swish'))\n",
    "model.add(Dense(64, activation='swish'))\n",
    "model.add(Dense(64, activation='swish'))\n",
    "model.add(Dense(64, activation='swish'))\n",
    "model.add(Dense(64, activation='swish'))\n",
    "model.add(Dense(32, activation='swish'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c890d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  critic_net = critic_network.CriticNetwork(\n",
    "        (observation_spec, action_spec),\n",
    "        \n",
    "        # INPUT = observation (depth sensor image 240x320 pix)\n",
    "        # Conv2D(filters, kernel size, stride)\n",
    "        observation_conv_layer_params=((16,7,4), (8,5,2), (1,3,1)),\n",
    "        # Dense(number_of_units)\n",
    "        observation_fc_layer_params=(128, 64, 64, 64, 64, 64, 64, 32),\n",
    "        # Dropout(rate) dropout layer is after each fully connected layer\n",
    "        observation_dropout_layer_params=(0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25),\n",
    "\n",
    "        \n",
    "        # INPUT = actions\n",
    "        # Dense(number_of_units)\n",
    "        action_fc_layer_params=(64, 64, 64, 64, 64, 32),\n",
    "        # Dropout(rate) dropout layer is after each fully connected layer\n",
    "        action_dropout_layer_params=(0.25, 0.25, 0.25, 0.25, 0.25, 0.25),\n",
    "        \n",
    "      \n",
    "        # INPUT = [observation, action]\n",
    "        # Dense(number_of_units) \n",
    "        joint_fc_layer_params=(32, 32, 32, 32, 32),\n",
    "        # Dropout(rate) dropout layer is after each fully connected layer\n",
    "        joint_dropout_layer_params=(0.25, 0.25, 0.25, 0.25, 0.25),\n",
    "        # activation function for all layers (conv2D and Dense)\n",
    "        activation_fn=tf.keras.activations.swish, \n",
    "        output_activation_fn=None,\n",
    "        name='CriticNetwork')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c660a2",
   "metadata": {},
   "source": [
    "### Actor\n",
    "Generates actions for given observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "198ebff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "        observation_spec,\n",
    "        action_spec,\n",
    "        preprocessing_layers = None,\n",
    "        preprocessing_combiner=None, \n",
    "        conv_layer_params=((16,7,4), (8,5,2), (1,3,1)),\n",
    "        fc_layer_params=(128, 64, 64, 64, 64, 64, 64, 64), \n",
    "        dropout_layer_params=(0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25),\n",
    "        kernel_initializer=None,\n",
    "        activation_fn=tf.keras.activations.swish,\n",
    "        continuous_projection_net=tanh_normal_projection_network.TanhNormalProjectionNetwork,\n",
    "        name='ActorDistributionNetwork')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e01114f",
   "metadata": {},
   "source": [
    "### Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffe9960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  train_step = train_utils.create_train_step()\n",
    "  tf_agent = sac_agent.SacAgent(\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        critic_network=critic_net,\n",
    "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=0.00004), #0.00003\n",
    "        actor_network=actor_net,\n",
    "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=0.00005),\n",
    "        alpha_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=0.00002),\n",
    "        actor_loss_weight = 1.0,\n",
    "        critic_loss_weight = 0.5,\n",
    "        alpha_loss_weight = 0.8,\n",
    "        target_update_tau=0.02,\n",
    "        target_update_period=1,\n",
    "        td_errors_loss_fn=tf.math.squared_difference,\n",
    "        gamma=0.98,\n",
    "        reward_scale_factor=1.0,\n",
    "        initial_log_alpha = 0.1,\n",
    "        use_log_alpha_in_alpha_loss = False,\n",
    "        target_entropy = -0.1,\n",
    "        gradient_clipping = None,\n",
    "        debug_summaries = True,\n",
    "        summarize_grads_and_vars = False,\n",
    "        train_step_counter=train_step,\n",
    "        name='Agent')\n",
    "\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45856d11",
   "metadata": {},
   "source": [
    "### Critic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fbb658f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CriticNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "observation_encoding/conv2d  multiple                  800       \n",
      "_________________________________________________________________\n",
      "observation_encoding/conv2d  multiple                  3208      \n",
      "_________________________________________________________________\n",
      "observation_encoding/conv2d  multiple                  73        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "observation_encoding/dense ( multiple                  119936    \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "observation_encoding/dense ( multiple                  8256      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "observation_encoding/dense ( multiple                  4160      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "observation_encoding/dense ( multiple                  4160      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "observation_encoding/dense ( multiple                  4160      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "observation_encoding/dense ( multiple                  4160      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "observation_encoding/dense ( multiple                  4160      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "observation_encoding/dense ( multiple                  2080      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "action_encoding/dense (Dense multiple                  192       \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "action_encoding/dense (Dense multiple                  4160      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "action_encoding/dense (Dense multiple                  4160      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "action_encoding/dense (Dense multiple                  4160      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "action_encoding/dense (Dense multiple                  4160      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "action_encoding/dense (Dense multiple                  2080      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "joint_mlp/dense (Dense)      multiple                  2080      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "joint_mlp/dense (Dense)      multiple                  1056      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "joint_mlp/dense (Dense)      multiple                  1056      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "joint_mlp/dense (Dense)      multiple                  1056      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "joint_mlp/dense (Dense)      multiple                  1056      \n",
      "_________________________________________________________________\n",
      "permanent_variable_rate_drop multiple                  0         \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  33        \n",
      "=================================================================\n",
      "Total params: 180,402\n",
      "Trainable params: 180,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e288035c",
   "metadata": {},
   "source": [
    "### Actor net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11852a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ActorDistributionNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncodingNetwork (EncodingNet multiple                  157233    \n",
      "_________________________________________________________________\n",
      "TanhNormalProjectionNetwork  multiple                  260       \n",
      "=================================================================\n",
      "Total params: 157,493\n",
      "Trainable params: 157,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e7bd6",
   "metadata": {},
   "source": [
    "# Observer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "433d2dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Agents metrics as Observer assistant\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(buffer_size=episodes_in_patch),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(buffer_size=episodes_in_patch)]\n",
    "\n",
    "# Custom observer for Tensorboard logging\n",
    "class Observer:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Save episode count between kernel restarts to keep Tensorboard graphs from resetting step count\n",
    "        self.episode_file = log_dir+'TB_episodes.txt'\n",
    "        \n",
    "        # Initialize writer with class to avoid empty log files\n",
    "        self.summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "        \n",
    "        try:\n",
    "            # Read episode count (in previous training) from temp file\n",
    "            self.episodes = int(open(self.episode_file, 'r').read())\n",
    "        \n",
    "        except:\n",
    "            # If file not found\n",
    "            self.episodes = 0\n",
    "            \n",
    "            \n",
    "    def __call__(self, trajectory):\n",
    "        \n",
    "        # Values from Tensorflow train_metrics\n",
    "        current_episode = train_metrics[0].result().numpy()\n",
    "        current_steps = train_metrics[1].result().numpy()\n",
    "        avg_return = train_metrics[2].result().numpy()\n",
    "        avg_steps = train_metrics[3].result().numpy()\n",
    "        \n",
    "        with self.summary_writer.as_default(step = (self.episodes + current_episode)):             \n",
    "            \n",
    "            # Store summaries after each patch\n",
    "            if current_episode % episodes_in_patch == 0:\n",
    "                \n",
    "                # Last episode count written to Tensorboard is stored in file\n",
    "                open(self.episode_file, 'w+').write(str(self.episodes + current_episode))\n",
    "                \n",
    "                # Write to Tensorboard log folder\n",
    "                tf.summary.scalar(f'Avg Reward per episode', avg_return)\n",
    "                tf.summary.scalar(f'Avg Steps per episode', avg_steps)\n",
    "            \n",
    "            # Show live step and episode count after each step\n",
    "            print(f\"\\rTotal steps: {current_steps} in {current_episode} episodes\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a2b8b4",
   "metadata": {},
   "source": [
    "# Replay buffer\n",
    "\n",
    "Store data about previous training experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65e1bd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create replay buffer (step database)\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec = tf_agent.collect_data_spec,\n",
    "    batch_size = 1,                 # Replays are stored one at the time\n",
    "    max_length = replay_max_length, # Total buffer size\n",
    "    device='cpu:0',                 # Use CPU for data storage and compute (GPU needed for network training)\n",
    "    dataset_drop_remainder=True,    \n",
    "    dataset_window_shift=None, \n",
    "    stateful_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a740b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Loaded 13859 steps into buffer from data/real_buffer/\n"
     ]
    }
   ],
   "source": [
    "if buffer_from_file:\n",
    "    \n",
    "    # Try loading replay buffer seed from a file\n",
    "    try:\n",
    "        tf.train.Checkpoint(replay_buffer=replay_buffer).restore(rb_tempdir+ '-1')\n",
    "        logging.info(f\"Loaded {replay_buffer.gather_all().action[0].shape[0]} steps into buffer from {rb_tempdir}\")\n",
    "    \n",
    "    except:\n",
    "        logging.info(f\"No previous replay buffer steps found in {rb_tempdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c866293",
   "metadata": {},
   "source": [
    "#### Reading data from buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "685b322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from replay buffer\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=sample_batch_size,\n",
    "    num_parallel_calls = 1,\n",
    "    num_steps=2).prefetch(training_iterations)\n",
    "\n",
    "experience_dataset_fn = lambda: dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06404998",
   "metadata": {},
   "source": [
    "# Tensorboard\n",
    "\n",
    "For model metrics run in terminal: **tensorboard --logdir=models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99593153",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3b7cd84926c844b9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3b7cd84926c844b9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:For model metrics run in terminal: tensorboard --logdir=models\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n",
    "logging.info(f\"For model metrics run in terminal: tensorboard --logdir=models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732554ca",
   "metadata": {},
   "source": [
    "# Random training\n",
    "\n",
    "To get baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57f52d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Using replay from file. No random steps\n"
     ]
    }
   ],
   "source": [
    "if random_replay_fill:\n",
    "    # Folder for random policy logs\n",
    "    log_dir = 'logs/RANDOM'\n",
    "\n",
    "    # Real and Sim in different subfolders\n",
    "    if real:\n",
    "        log_dir += '/REAL/'\n",
    "    else:\n",
    "        log_dir += '/SIM/'\n",
    "    \n",
    "    \n",
    "    # Use random policy\n",
    "    initial_collect_policy = random_tf_policy.RandomTFPolicy(action_spec = tf_env.action_spec(),\n",
    "                                                          time_step_spec = tf_env.time_step_spec())\n",
    "    # Use episode driver\n",
    "    inital_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        tf_env, \n",
    "        initial_collect_policy, \n",
    "        observers = [replay_buffer.add_batch, Observer()] + train_metrics, \n",
    "        num_steps = 1000)\n",
    "    \n",
    "    for _ in range(round(replay_max_length/1000)):\n",
    "        \n",
    "        try:\n",
    "            # Do 1000 steps\n",
    "            inital_driver.run()\n",
    "        except:\n",
    "            logging.info(f\"Exception occured: {sys.exc_info()[0]}\")\n",
    "            logging.info(\"Saving replay buffer!\")\n",
    "            logging.info(f\"Saved replay buffer at {replay_buffer.gather_all().action[0].shape[0]} steps\")\n",
    "    \n",
    "    # Save replay buffer\n",
    "    tf.train.Checkpoint(replay_buffer=replay_buffer).save(rb_tempdir)\n",
    "    logging.info(f\"Saved replay buffer at {replay_buffer.gather_all().action[0].shape[0]} steps\")\n",
    "else:\n",
    "    logging.info(f\"Using replay from file. No random steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d1b799",
   "metadata": {},
   "source": [
    "# Collect driver\n",
    "\n",
    "Driver for running a policy in an environment. Does steps until num_episodes episodes is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4b8e83",
   "metadata": {},
   "source": [
    "### Tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ea69036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder for training logs\n",
    "log_dir = 'logs/' + experiment_name\n",
    "\n",
    "# Place logs in env type subfolder\n",
    "if real:\n",
    "    log_dir += '/REAL/'\n",
    "else:\n",
    "    log_dir += '/SIM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63ccfc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use episode driver (nr of steps per episode is limited by Gym env)\n",
    "collect_actor = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env, \n",
    "    py_tf_eager_policy.PyTFEagerPolicy(tf_agent.collect_policy, use_tf_function=True),\n",
    "    observers = [replay_buffer.add_batch, Observer()] + train_metrics,\n",
    "    num_episodes = episodes_in_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39db5603",
   "metadata": {},
   "source": [
    "# Learner\n",
    "\n",
    "Learner loads checkpoint from tempdir if available from previous learning session. Learning will be resumed from saved point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af62f915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Checkpoint available: models/Macro_net/train/checkpoints/ckpt-1135750\n"
     ]
    }
   ],
   "source": [
    "agent_learner = learner.Learner(\n",
    "    tempdir,                               \n",
    "    train_step, \n",
    "    tf_agent,                               \n",
    "    experience_dataset_fn,                  \n",
    "    checkpoint_interval=training_iterations,\n",
    "    summary_interval=training_iterations, \n",
    "    max_checkpoints_to_keep=2,\n",
    "    strategy=strategy,\n",
    "    run_optimizer_variable_init=True)\n",
    "\n",
    "# DEBUG: ckpt-xxxx <-shows how many training iterations has been done previously for current experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe00c8a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9482ae0d",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02296a27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, patches_to_run+1):\n",
    "\n",
    "    try:\n",
    "        # Collecting delay is used for enabling pretraining on previously saved replay buffer\n",
    "        if i > start_collecting:\n",
    "            for _ in range(times_to_collect):\n",
    "                collect_actor.run()\n",
    "                   \n",
    "            # Save replay buffer into data folder file\n",
    "            if i % buffer_save_interval == 0 and save_buffer_to_file:\n",
    "                tf.train.Checkpoint(replay_buffer=replay_buffer).save(rb_tempdir)\n",
    "                logging.info(f\"Saved replay buffer at {replay_buffer.gather_all().action[0].shape[0]} steps\")\n",
    "     \n",
    "        \n",
    "        # Only training when algorithm mode = TRAIN. Learning is disabled when .TEST -ing\n",
    "        if  algorithm_mode == AlgorithmMode.TRAIN:         \n",
    "            for _ in range(times_to_iterate):\n",
    "                agent_learner.run(iterations=training_iterations)\n",
    "\n",
    "    except:\n",
    "        logging.info(f\"Exception occured: {sys.exc_info()[0]}\")\n",
    "        \n",
    "        # Close env to save replay buffer faster for sim env\n",
    "        if real == False:\n",
    "            tf_env.close()\n",
    "        \n",
    "        # When session ends, on exception or kernerl interrupt: save latest buffer state to a file\n",
    "        if save_buffer_to_file:\n",
    "            logging.info(\"Exception occured! Saving last buffer state to a file.\")\n",
    "            tf.train.Checkpoint(replay_buffer=replay_buffer).save(rb_tempdir)\n",
    "            logging.info(f\"Saved replay buffer at {replay_buffer.gather_all().action[0].shape[0]} steps\")\n",
    "        \n",
    "        # Exit loop\n",
    "        break\n",
    "        \n",
    "logging.info(\"Training stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
